{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime\n",
    "from sdv.single_table import CTGANSynthesizer,TVAESynthesizer\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sklearn import preprocessing,metrics\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Func\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas_gbq\n",
    "import json\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "from imblearn.over_sampling import SMOTE,SMOTENC\n",
    "from sklearn.svm import OneClassSVM\n",
    "import statistics\n",
    "from catboost import CatBoostClassifier\n",
    "import os\n",
    "\n",
    "os.environ['SCIPY_ARRAY_API'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"finops-55-0c8185e843cd.json\",\"r\") as f:\n",
    "    json_data=json.load(f)\n",
    "f.close()\n",
    "cred = service_account.Credentials.from_service_account_info(json_data)\n",
    "client = bigquery.Client(project=json_data['project_id'],credentials=cred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = (\n",
    "    '''SELECT * from `finops-55.billing_1.synthesized_data_pseudo_label3_thres_51`''' )\n",
    "query_job = client.query(QUERY)  # API request\n",
    "df_model=query_job.to_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable before importing sklearn or scipy\n",
    "os.environ['SCIPY_ARRAY_API'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model['is_anomaly'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_all(df):\n",
    "    for i in df.select_dtypes(include='object').columns:\n",
    "        df[i] = df[i].str.lower()\n",
    "        if df[i].isnull().sum()/len(df)>0 and i not in ['regionname','servicecategory']:\n",
    "            print(i,df[i].isnull().sum())\n",
    "            df[i].fillna(df.groupby(['cloud','servicename'])[i].transform(statistics.mode),inplace=True)\n",
    "        elif df[i].isnull().sum()/len(df)>0 and i=='regionname':\n",
    "            df[i].fillna(statistics.mode(df[i]),inplace=True)\n",
    "        elif df[i].isnull().sum()/len(df)>0 and i=='servicecategory':\n",
    "            df[i].fillna('Other',inplace=True)\n",
    "    for i in df.select_dtypes(include=np.number).columns:\n",
    "        if df[i].isnull().sum()/len(df)>0:\n",
    "            df[i].fillna(df.groupby(['cloud','servicename'])[i].transform('mean'),inplace=True)\n",
    "    return df\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model['listcost'].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model['listcost'].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_model['is_anomaly']\n",
    "x = df_model.drop('is_anomaly',axis=1)\n",
    "for i in ['chargeperiodstart_month', 'chargeperiodstart_day','chargeperiodstart_year']:\n",
    "    x[i] = x[i].astype('str')\n",
    "x_temp, x_test1, y_temp, y_test = train_test_split(x, y, test_size=0.2, random_state=42,stratify=y)\n",
    "x_train1, x_val1, y_train1, y_val = train_test_split(x_temp, y_temp, test_size=0.125, random_state=42,stratify=y_temp)\n",
    "# print(x_train.shape,x_val1.shape,x_test1.shape)\n",
    "# overlap = pd.merge(x_train, x_test, how='inner')\n",
    "# mask = x_train.apply(tuple, axis=1).isin(overlap.apply(tuple, axis=1))\n",
    "# x_train1 = x_train[~mask]\n",
    "# y_train1 = y_train[~mask]\n",
    "x_train_scaled = preprocessing_all(x_train1)\n",
    "x_val_scaled = preprocessing_all(x_val1)\n",
    "x_test_scaled = preprocessing_all(x_test1)\n",
    "# print(x_train_scaled.shape,x_val_scaled.shape,x_test_scaled.shape)\n",
    "x_test=x_test1.copy()\n",
    "x_val=x_val1.copy()\n",
    "x_train = x_train1.copy()\n",
    "y_train = y_train1.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and balancing\n",
    "print(x_train_scaled.shape,x_test_scaled.shape,x_val_scaled.shape)\n",
    "encoder = OneHotEncoder(handle_unknown='ignore',sparse_output=False)\n",
    "x_train_cat = encoder.fit_transform(x_train_scaled.select_dtypes(include=\"object\"))\n",
    "x_train_cat = pd.DataFrame(x_train_cat, columns=encoder.get_feature_names_out(x_train_scaled.select_dtypes(include=\"object\").columns),index=x_train_scaled.index)\n",
    "print(x_train_cat.shape)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "train_scaled_num = scaler.fit_transform(x_train_scaled.select_dtypes(include=np.number))\n",
    "print(train_scaled_num.shape)\n",
    "train_scaled_num = pd.DataFrame(train_scaled_num,columns=x_train_scaled.select_dtypes(include=np.number).columns,index=x_train_scaled.index)\n",
    "print(train_scaled_num.shape)\n",
    "x_train_balanced = pd.concat([train_scaled_num,x_train_cat],axis=1)\n",
    "x_train_balanced = x_train_balanced.fillna(0)\n",
    "print(x_train_balanced.shape)\n",
    "smote = SMOTE(sampling_strategy='minority',random_state=0,k_neighbors=100)\n",
    "x_train_smote,y_train_smote= smote.fit_resample(x_train_balanced, y_train)\n",
    "print(x_train_smote.shape)\n",
    "df_model_balanced_train = pd.concat([x_train_balanced,y_train],axis=1)\n",
    "x_train = df_model_balanced_train.drop('is_anomaly',axis=1)\n",
    "y_train = df_model_balanced_train['is_anomaly']\n",
    "print(x_train_scaled.shape,x_test_scaled.shape,x_val_scaled.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(x_train_scaled.columns)-set(x_test_scaled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Val\n",
    "x_val_cat = encoder.transform(x_val_scaled.select_dtypes(include=\"object\"))\n",
    "x_val_cat = pd.DataFrame(x_val_cat, columns=encoder.get_feature_names_out(x_val_scaled.select_dtypes(include=\"object\").columns),index=x_val_scaled.index)\n",
    "val_scaled = scaler.transform(x_val_scaled.select_dtypes(include=np.number))\n",
    "x_val_num = pd.DataFrame(val_scaled, columns = x_val_scaled.select_dtypes(include=np.number).columns,index=x_val_scaled.index)\n",
    "x_val = pd.concat([x_val_cat,x_val_num],axis=1)\n",
    "\n",
    "#Test\n",
    "x_test_cat = encoder.transform(x_test_scaled.select_dtypes(include=\"object\"))\n",
    "x_test_cat = pd.DataFrame(x_test_scaled, columns=encoder.get_feature_names_out(x_test_scaled.select_dtypes(include=\"object\").columns),index=x_test_scaled.index)\n",
    "test_scaled = scaler.transform(x_test_scaled.select_dtypes(include=np.number))\n",
    "x_test_num = pd.DataFrame(test_scaled, columns = x_test_scaled.select_dtypes(include=np.number).columns,index=x_test_scaled.index)\n",
    "x_test = pd.concat([x_test_cat,x_test_num],axis=1)\n",
    "print(x_train.shape,x_val.shape,x_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = x_train.columns\n",
    "x_test = x_test.reindex(columns=order)\n",
    "x_val = x_val.reindex(columns=order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=PCA(n_components=287)\n",
    "x_train,x_test,x_val=x_train.fillna(0),x_test.fillna(0),x_val.fillna(0)\n",
    "x_train_pca=pca.fit_transform(x_train)\n",
    "x_test_pca=pca.transform(x_test)\n",
    "x_val_pca = pca.transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=0)\n",
    "logreg.fit(x_train_pca, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = logreg.predict(x_train_pca)\n",
    "y_pred = logreg.predict(x_test_pca)\n",
    "y_pred_prob = logreg.predict_log_proba(x_test_pca)[:, 1]\n",
    "# Evaluation\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotcon = ConfusionMatrixDisplay(confusion_matrix=metrics.confusion_matrix(y_test,y_pred),display_labels=logreg.classes_)\n",
    "plotcon.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "kf = KFold(shuffle=True,n_splits=5,random_state=0)\n",
    "score = cross_val_score(logreg,x_test_pca,y_test,cv=kf,scoring='roc_auc')\n",
    "np.mean(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse, bias, variance = bias_variance_decomp(\n",
    "    logreg, np.array(x_train_pca), np.array(y_train), np.array(x_test_pca), np.array(y_test),\n",
    "    loss='0-1_loss',  # Classification error\n",
    "    num_rounds=100, \n",
    "    random_seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse, bias, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Calculate FPR, TPR, and thresholds\n",
    "fpr_lg, tpr_lg, thresholds_lg = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Calculate the AUC score\n",
    "roc_auc_lg = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_lg, tpr_lg, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc_lg:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random Guessing')\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_rf.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = model_rf.predict(x_test_pca)\n",
    "y_pred_train_rf  =model_rf.predict(x_train_pca)\n",
    "y_pred_prob_rf = model_rf.predict_proba(x_test_pca)[:, 1]\n",
    "\n",
    "# Calculate accuracy\n",
    "\n",
    "# Print a classification report\n",
    "print(\"Classification Report: Train\")\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "print(\"Classification Report Test\")\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotcon = ConfusionMatrixDisplay(confusion_matrix=metrics.confusion_matrix(y_test,y_pred_rf),display_labels=model_rf.classes_)\n",
    "plotcon.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "kf = KFold(shuffle=True,n_splits=5,random_state=0)\n",
    "score = cross_val_score(model_rf,x_test_pca,y_test,cv=kf,scoring='roc_auc')\n",
    "np.mean(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_rf = model_rf.predict_proba(x_test_pca)[:, 1]\n",
    "\n",
    "# Calculate FPR, TPR, and thresholds\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_pred_prob_rf)\n",
    "\n",
    "# Calculate the AUC score\n",
    "roc_auc_rf = roc_auc_score(y_test, y_pred_prob_rf)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_rf, tpr_rf, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc_rf:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random Guessing')\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve for Random Forest')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.savefig('roc_curve_rf.png')\n",
    "\n",
    "print(f\"The AUC score is: {roc_auc_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "kf = KFold(shuffle=True,n_splits=5,random_state=0)\n",
    "score = cross_val_score(model_rf,x_test_pca,y_test,cv=kf,scoring='roc_auc')\n",
    "np.mean(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse, bias, variance = bias_variance_decomp(\n",
    "#     model_rf,X_train=np.array(x_train_pca), y_train=np.array(y_train), X_test=np.array(x_test_pca), y_test=np.array(y_test),\n",
    "#     loss='0-1_loss',  # Classification error\n",
    "#     num_rounds=10, \n",
    "#     random_seed=42\n",
    "# )\n",
    "# mse, bias, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50,75,100,125,150,200],\n",
    "    'max_depth': [10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=model_rf, param_grid=param_grid, scoring='recall')\n",
    "grid_search.fit(x_val_pca, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_grid = grid_search.predict(x_test_pca)\n",
    "print(classification_report(y_test,y_pred_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=200,weights='distance')\n",
    "knn.fit(x_train_pca, y_train)\n",
    "\n",
    "y_pred_knn = knn.predict(x_test_pca)\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "print(metrics.confusion_matrix(y_test,y_pred_knn))\n",
    "y_pred_prob_knn = knn.predict_proba(x_test_pca)[:, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotcon = ConfusionMatrixDisplay(confusion_matrix=metrics.confusion_matrix(y_test,y_pred_knn),display_labels=knn.classes_)\n",
    "plotcon.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse, bias, variance = bias_variance_decomp(\n",
    "    knn, np.array(x_train_pca), np.array(y_train), np.array(x_test_pca), np.array(y_test),\n",
    "    loss='0-1_loss',  # Classification error\n",
    "    num_rounds=10, \n",
    "    random_seed=42\n",
    ")\n",
    "mse, bias, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_knn = knn.predict_proba(x_test_pca)[:, 1]\n",
    "\n",
    "# Calculate FPR, TPR, and thresholds\n",
    "fpr_knn, tpr_knn, thresholds = roc_curve(y_test, y_pred_prob_knn)\n",
    "\n",
    "# Calculate the AUC score\n",
    "roc_auc_knn = roc_auc_score(y_test, y_pred_prob_knn)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_knn, tpr_knn, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc_knn:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random Guessing')\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve for KNN')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.savefig('roc_curve_rf.png')\n",
    "\n",
    "print(f\"The AUC score is: {roc_auc_knn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = x_train1.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "for i in categorical_features:\n",
    "    x_train1[i]=x_train1[i].astype('str')\n",
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_catboost = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.0001,\n",
    "    depth=6,\n",
    "    loss_function='Logloss', \n",
    "    eval_metric='Recall',         \n",
    "    random_seed=42,\n",
    "    verbose=0,\n",
    "    cat_features = categorical_features\n",
    ")\n",
    "\n",
    "model_catboost.fit(x_train1.fillna(''),y_train)\n",
    "y_pred_cb = model_catboost.predict(x_test1.fillna(''))\n",
    "y_pred_train = model_catboost.predict(x_train1.fillna(''))\n",
    "y_pred_prob_cat = model_catboost.predict_proba(x_test1.fillna(''))[:, 1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "importances = model_catboost.get_feature_importance(prettified=True)\n",
    "print(importances.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_cb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotcon = ConfusionMatrixDisplay(confusion_matrix=metrics.confusion_matrix(y_test,y_pred_cb),display_labels=model_catboost.classes_)\n",
    "plotcon.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse, bias, variance = bias_variance_decomp(\n",
    "#     model_catboost, np.array(x_train1), np.array(y_train), np.array(x_test1), np.array(y_test),\n",
    "#     loss='0-1_loss',  # Classification error\n",
    "#     num_rounds=10, \n",
    "#     random_seed=42\n",
    "# )\n",
    "# mse, bias, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_cb = model_catboost.predict_proba(x_test1.fillna(''))[:, 1]\n",
    "\n",
    "# Calculate FPR, TPR, and thresholds\n",
    "fpr_cb, tpr_cb, thresholds_cb = roc_curve(y_test, y_pred_prob_cb)\n",
    "\n",
    "# Calculate the AUC score\n",
    "roc_auc_cb = roc_auc_score(y_test, y_pred_prob_cb)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_cb, tpr_cb, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc_cb})')\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random Guessing')\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve for CatBoost')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.savefig('roc_curve_rf.png')\n",
    "\n",
    "print(f\"The AUC score is: {roc_auc_cb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_cb, tpr_cb, color='blue', lw=2, label=f'ROC curve CB(AUC = {round(roc_auc_cb,3)})')\n",
    "plt.plot(fpr_lg, tpr_lg, color='green', lw=2, label=f'ROC curve LR(AUC = {roc_auc_lg:.2f})')\n",
    "plt.plot(fpr_knn, tpr_knn, color='cyan', lw=2, label=f'ROC curve KNN(AUC = {roc_auc_knn:.2f})')\n",
    "plt.plot(fpr_rf, tpr_rf, color='purple', lw=2, label=f'ROC curve RF(AUC = {roc_auc_rf:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random Guessing')\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve for Classifiers')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.savefig('roc_curve_rf.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tuples = set(map(tuple, x_train.values))\n",
    "test_tuples  = set(map(tuple, x_test.values))\n",
    "overlap = train_tuples & test_tuples\n",
    "print(f\"Number of overlapping rows: {len(overlap)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([x_test1,pd.DataFrame({'is_anomaly':y_pred},index=x_test1.index)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_for_recom = result[(result['cloud']=='gcp') & (result['servicename']=='compute engine') & (result['is_anomaly']==1)].sort_values(by='listcost',ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_input=input_for_recom.to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input_data_for_recomm.json\", \"w\") as f:\n",
    "        json.dump(data_for_input, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
